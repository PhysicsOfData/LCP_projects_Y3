{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 \n",
    "### Milocco Riccardo\n",
    "\n",
    "The csv files are originated from different sources, hence resulting in differences in the encoding and end-of-lines that have to be taken into account in the data preparation phase. Make sure each .csv file is properly interpreted: Difference in the encoding and end-of-lines; -cvs file properly interpreted\n",
    "\n",
    "\n",
    "First, open the files, check the number of lines to be imported and the difference in encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Importing the required libraries.\n",
    "import pandas as pd #for dataframes and reading csv files.\n",
    "import numpy as np # for math operations(sqrt)\n",
    "import matplotlib.pyplot as plt #for plotting the graphs\n",
    "import IPython\n",
    "from IPython.display import display, HTML\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "import csv\n",
    "\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "files_indir = [\"data/\"+f for f in listdir(\"data\") if isfile(join(\"data\", f))]\n",
    "print(files_indir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd data; ! wc -l * >> numb_lines.txt\n",
    "!cd data; ! wc -l *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_dic = {}\n",
    "\n",
    "with open(\"data/numb_lines.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        (val, keys) = line.split()\n",
    "        len_dic[keys] = int(val)\n",
    "print(len_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd data; head -n 5 *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing \"day_od.cvs\", we applied the following considerations:\n",
    "* ```name``` = adding headers to columns; \n",
    "* ```encoding=utf-16``` for byte 0xfe at the start; \n",
    "* ```quoting=2``` for \"Non-numerical\" quoting;\n",
    "* not used ```usecols```, for explicitic specific ordering;\n",
    "* ```.astype(Int64)``` handels with NaN values\n",
    "* drop ```Origin``` and ```Destination``` columns since we don't have a specific labelling\n",
    "* defining```equal_len``` of the DataFrame to be compared with the previous analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe from day_od.csv: \"day origin and destination\"\n",
    "\n",
    "#name=None explicetely since adding column after that; utf for byte 0xfe at the start; quoting=2 \"Non-numerical\"\n",
    "#don't used usecols since we want a specific ordering + \"Int64\" handels with NaN values\n",
    "#drop Origin and Destination since we don't have a specific labelling of Origin and Destination\n",
    "\n",
    "file_name=\"data/day_od.csv\"\n",
    "day_od=pd.read_csv(file_name,sep=\",\",encoding='utf-16', \n",
    "names=[\"MONTH\",\"DOW\",\"ORIGIN\",\"DESTINATION\",\"CUST_CLASS\",\"COD_COUNTRY\",\"COD_PRO\",\"PRO_COM\",\"FLOW\"], \n",
    "header=0, quoting=2, quotechar='\"') #comment: explicit header=0 to replace column names;  \n",
    "\n",
    "#************PREVIOUSLY THERE WAS THIS CONVERSION. THINK IT'S NOT USEFUL\n",
    "#day_od=pd.DataFrame(read_day_od)#dataFrame of the file(can be read more easily than csv)\n",
    "\n",
    "equal_len=[]\n",
    "equal_len.append(len(day_od)+1 == len_dic[file_name.replace(\"data/\",\"\")])\n",
    "day_od[ [\"COD_COUNTRY\",\"COD_PRO\",\"PRO_COM\"] ]=day_od[ [\"COD_COUNTRY\",\"COD_PRO\",\"PRO_COM\"] ].astype(\"Int64\") #Int64 handles NaN\n",
    "day_od.drop([\"ORIGIN\",\"DESTINATION\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Importing this .cvs with:\n",
    "* ```encoding=\"latin-1\"``` since last letters of the days present accent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read distinct_users_day.csv\n",
    "file_name=\"data/distinct_users_day.csv\"\n",
    "dist_users_day=pd.read_csv(file_name,sep=\",\",encoding=\"latin-1\")\n",
    "#dist_users_day=pd.DataFrame(read_users_day)\n",
    "equal_len.append(len(dist_users_day)+1 == len_dic[file_name.replace(\"data/\",\"\")])\n",
    "\n",
    "dist_users_day[[\"COD_COUNTRY\",\"COD_PRO\",\"PRO_COM\"]]=dist_users_day[ [\"COD_COUNTRY\",\"COD_PRO\",\"PRO_COM\"] ].astype(\"Int64\") #Int64 handles NaN\n",
    "\n",
    "#sorting...\n",
    "dist_users_day[dist_users_day[\"CUST_CLASS\"]==\"visitor\"].groupby(\"COD_PRO\", as_index=False).sum().sort_values(\"VISITORS\", ascending=False)\n",
    "\n",
    "dist_users_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Getting DataFrame for codici_istat_comune.csv\n",
    "file_name=\"data/codici_istat_comune.csv\"\n",
    "codist_com=pd.read_csv(file_name,sep=\",\",delimiter=',',header=0,encoding=\"latin-1\")\n",
    "equal_len.append(len(codist_com)+1 == len_dic[file_name.replace(\"data/\",\"\")])\n",
    "#codist_com=pd.DataFrame(data_cic)\n",
    "codist_com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting DataFrame for codici_istat_provincia.csv\n",
    "file_name=\"data/codici_istat_provincia.csv\"\n",
    "codist_prov=pd.read_csv(file_name,sep=\",\",encoding=\"latin-1\",header=0)\n",
    "equal_len.append(len(codist_prov)+1 == len_dic[file_name.replace(\"data/\",\"\")])\n",
    "#codist_prov=pd.DataFrame(data_cip)\n",
    "codist_prov.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Added:\n",
    "* ```quotechar=2```;\n",
    "* ```escapechar=\"\\r\"``` since ```\\r\\n``` will also ```\\r``` character in ```COUNTRY_NAME_IT```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting DataFrame for codici_nazioni.csv\n",
    "#utf16 since there's byte 0xfe at the start, \" as quoting, escapechar=\"\\r\" since endline=\\n\n",
    "import csv\n",
    "file_name=\"data/codici_nazioni.csv\"\n",
    "codist_naz=pd.read_csv(file_name,delimiter=',',header=0,quoting=2, #quoting=2 \"Non-Numerical value ex. \"Colombia\";\n",
    "                                error_bad_lines=True,encoding=\"utf16\",quotechar = '\"',escapechar='\\r')\n",
    "\n",
    "#codist_naz=pd.DataFrame(data_cin)\n",
    "\n",
    "codist_naz[ [\"COD_COUNTRY\"] ]=codist_naz[ [\"COD_COUNTRY\"] ].astype(\"Int64\") #Int64 handles NaN\n",
    "\n",
    "equal_len.append(len(codist_naz)+1 == len_dic[file_name.replace(\"data/\",\"\")])\n",
    "\n",
    "value=658\n",
    "def color658(val):\n",
    "    color = 'red' if val == value else 'white'\n",
    "    return 'background-color: %s' % color\n",
    "s = codist_naz[107:110].style.applymap(color658) #applymap element wise but apply along all axis\n",
    "s\n",
    "\n",
    "#index_series=codist_naz[codist_naz.COD_COUNTRY==658].index\n",
    "#index_series[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To correct interpret the decimal numbers written with ```,``` two ways are possible:\n",
    "\n",
    "* ```cat Veneto.txt | tr ',' '.' > Veneto_fixed.txt```\n",
    "* use explicitely ```decimals=\",\"``` in ```.read_csv```\n",
    "\n",
    "Having done this, the ```Origin``` and ```Destination``` columns are ready to be converted in Int or just dropped as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting DataFrame for veneto.txt\n",
    "url=r\"data/Veneto.txt\"\n",
    "codist_ven=pd.read_csv(url,delimiter=';',decimal=\",\",header=0,quoting=2,quotechar = '\"',encoding=\"utf8\",error_bad_lines=True)\n",
    "#codist_ven=pd.DataFrame(data_veneto)\n",
    "equal_len.append(len(codist_ven)+1 == len_dic[url.replace(\"data/\",\"\")])\n",
    "\n",
    "codist_ven[ [\"Origine\",\"Destinazione\"] ]=codist_ven[ [\"Origine\",\"Destinazione\"] ].astype(\"Int64\")\n",
    "codist_ven"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if all the DataFrame lenghts are equal to the start esatimate of the lines of the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all(equal_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "\n",
    "### Ziliotto Filippo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from IPython.display import display, HTML\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objects as go\n",
    "import csv\n",
    "import geopandas\n",
    "from matplotlib import colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------Population by province-------------\n",
    "filename=r\"data/abitanti2.txt\"\n",
    "abitanti = pd.read_csv(filename, header=None,quoting=2,encoding=\"latin-1\")\n",
    "abitanti = pd.DataFrame(abitanti[0].str.split('\\t' ,n=1).tolist())\n",
    "abitanti.columns = [\"PROVINCIA\", \"VALUE\"]\n",
    "abitanti['VALUE'] = abitanti['VALUE'].apply(pd.to_numeric).sort_values(ascending=False)\n",
    "\n",
    "#-----------------file JSON of Geopandas------------\n",
    "province_italia_geojson = r'data\\limits_IT_provinces.geojson'\n",
    "pro_json = geopandas.read_file(province_italia_geojson)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ```abitanti2.txt``` is the file txt file corresponding to the population for each province\n",
    "\n",
    "* ```limits_IT_provinces``` is the file json taken from a github source that corresponds to the Pandas dataframe of the borders of italy (Regions, Provinces, municipalities).\n",
    "<https://github.com/openpolis/geojson-italy>\n",
    "\n",
    "* All other files where already read in the part above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting data\n",
    "\n",
    "The next cell displays two Pandas dataframes relate to the ranking of visitors after all data was cleaned and sorted out to tackle this pourpuse.\n",
    "* The dataset used for this part was ```distinct_users_day.csv```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------displaying dataframes side by side------\n",
    "def display_side(dfs:list, captions:list):\n",
    "    output = \"\"\n",
    "    combined = dict(zip(captions, dfs))\n",
    "    for caption, df in combined.items():\n",
    "        output += df.style.set_table_attributes(\"style='display:inline'\").set_caption(caption)._repr_html_()\n",
    "        output += \"\\xa0\\xa0\\xa0\"\n",
    "    display(HTML(output))\n",
    "\n",
    "#----------------Cleaning data and groupbying-----------\n",
    "def foreigner_visitors(dist_users_day):\n",
    "    \n",
    "    global visitors,visitors1\n",
    "    \n",
    "    visitors = dist_users_day[dist_users_day['CUST_CLASS'] == 'foreigner'\n",
    "                         ].merge(codist_naz,on='COD_COUNTRY'\n",
    "                         ).sort_values(by=['VISITORS'],ascending=False,ignore_index=True\n",
    "                         ).drop(columns=['COD_PRO','PRO_COM','CUST_CLASS'],axis=1\n",
    "                         ).dropna(axis='rows')\n",
    "        \n",
    "    visitors1 = visitors.groupby(['COUNTRY_NAME_IT'],as_index=False\n",
    "                            ).sum(\n",
    "                            ).sort_values(by=['VISITORS'],ascending=False,ignore_index=True\n",
    "                            ).drop(columns=['COD_COUNTRY'],axis=1\n",
    "                            ).copy()\n",
    "    \n",
    "    display_side([visitors.head(10), visitors1.head(10)], ['Cleaning', 'Groupby'])\n",
    "    \n",
    "foreigner_visitors(dist_users_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#-------------Displaying Graph------------------\n",
    "\n",
    "def graph_2_1():\n",
    "\n",
    "    fig = go.Figure()\n",
    "    config = {'displayModeBar': True}\n",
    "\n",
    "    df = visitors1[0:20]\n",
    "    fig = px.pie(df, values='VISITORS',\n",
    "                 names='COUNTRY_NAME_IT',\n",
    "                 title='Ranking of foreigner visitors weekly',\n",
    "                 hole=.25,\n",
    "                 color_discrete_sequence=px.colors.sequential.OrRd\n",
    "                )\n",
    "    \n",
    "    fig.update_traces(textposition='inside',\n",
    "                      hoverinfo='percent+value',\n",
    "                      textinfo='percent+label',\n",
    "                      textfont_size=12,\n",
    "                      marker=dict( line=dict(color='#000000', width=1.5)))\n",
    "\n",
    "    fig.update_layout(margin=dict(t=50, b=0, l=0, r=0))\n",
    "    fig.show(config=config)\n",
    "\n",
    "graph_2_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments\n",
    "\n",
    "- The graph represents the ranking of foreing visitors during a weekly period. The data was treated as a whole and not considering the single days of the week, also the n° of visitors has to be considered a sum of the weeks during a 4 month period from February to May.\n",
    "\n",
    "- The flux is not scaled by the n° of habitants of the countries in this graph. It was considered only the total number of foreinger people that passed in Padova and the relative flux.\n",
    "\n",
    "- It is possible to see that the contribute of the first 5 countries is $\\sim 50\\%$ of the total\n",
    "\n",
    "- In the number of visitors value, it should be taken into account that many people could be the same (and so counted many times) coming and going to Padova (let's for example think about foreinger students returning home)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part the goal was to analyze the different ranking of the flux of all provinces including Veneto's ones. The analysis was also differentiated by days of the week and months period.\n",
    "* The main datasets used for this part where ```day_od.csv``` & ```codici_istat_provincia.csv```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def provinces_visitors(day_od):\n",
    "    \n",
    "    #mapping upper character to columns to use dataframes\n",
    "    \n",
    "    day_od.columns = map(str.upper, day_od.columns)\n",
    "    \n",
    "    #defining global variables for plotting needs\n",
    "    \n",
    "    global day_od1,day_od2,day_od3,day_od_total\n",
    "\n",
    "    #selecting only visitors from italian provinces and cleaning data\n",
    "    \n",
    "    day_od1 = day_od[(day_od['COD_COUNTRY'] == 222) & (day_od['CUST_CLASS'] == 'visitor') & (day_od['COD_PRO'] != -999.0)\n",
    "                ].merge(codist_prov,how='left',on='COD_PRO'\n",
    "                ).drop(columns=['COD_REG','PROV_SIGLA','COD_COUNTRY','PRO_COM','ORIGIN','DESTINATION']\n",
    "                ).dropna(axis='rows'       \n",
    "                ).sort_values(by=['FLOW','PROVINCIA'],ascending=False, na_position='last',ignore_index=True\n",
    "                ).copy()\n",
    "\n",
    "    day_od2 = day_od1.groupby(['PROVINCIA','MONTH','DOW'],as_index=False\n",
    "                ).sum(\n",
    "                ).sort_values(by='FLOW',ascending=False,ignore_index=True\n",
    "                ).drop(columns=['COD_PRO']\n",
    "                )\n",
    "\n",
    "    #displaying side by side\n",
    "    pd.set_option('precision', 0)\n",
    "    display_side([day_od1.head(10),day_od2.head(10)],['Cleaning','Provincies'])\n",
    "    \n",
    "    #Merging on PROVINCIA to take only the total flux without differentiating by month or days\n",
    "    \n",
    "    day_od3 = day_od2.merge(abitanti,on='PROVINCIA')\n",
    "    day_od3['RATIO'] = day_od3['FLOW']/day_od3['VALUE']\n",
    "    day_od3.sort_values(by=['RATIO','PROVINCIA'],ascending=False)\n",
    "\n",
    "    pd.set_option('precision', 3)\n",
    "    day_od_total = day_od3.groupby(['PROVINCIA'], as_index=False)['RATIO'].sum(\n",
    "                ).sort_values(by='RATIO',ascending=False,ignore_index=True)\n",
    "    \n",
    "\n",
    "    display_side([abitanti.head(10),day_od3.head(10),day_od_total.head(10)]\n",
    "                ,['N° of inhabiants','Flow/Inhabitants','Total Flux'])\n",
    "    \n",
    "    return \n",
    "\n",
    "provinces_visitors(day_od)\n",
    "pd.reset_option('precision')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#------------------------Displaying Graph----------------------------------\n",
    "    \n",
    "def  graph_3_1():\n",
    "    \n",
    "    fig = go.Figure()\n",
    "\n",
    "    config = {'displayModeBar': True}\n",
    "\n",
    "    df = day_od_total[0:20]\n",
    "    fig = px.pie(\n",
    "                 df,\n",
    "                 values='RATIO', \n",
    "                 names='PROVINCIA', \n",
    "                 title='Ranking of provinces with most visitors by the total flux',\n",
    "                 hole=.0,\n",
    "                 color_discrete_sequence=px.colors.sequential.OrRd\n",
    "                )\n",
    "                \n",
    "\n",
    "    fig.update_traces(\n",
    "                 textposition='inside',\n",
    "                 hoverinfo='percent+label+value',\n",
    "                 textinfo='percent+label',\n",
    "                 textfont_size=13,\n",
    "                 marker=dict( line=dict(color='#010000', width=1.5)),\n",
    "                 pull=[0, 0, 0, 0,0,0,0,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2]\n",
    "                     )\n",
    "    \n",
    "    fig.update_traces( selector=dict(type='pie'))\n",
    "    fig.update_layout(margin=dict(t=50, b=0, l=0, r=0))\n",
    "    fig.show(config=config)\n",
    "    \n",
    "    return\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "\n",
    "def graph_3_2():\n",
    "\n",
    "    vis = pro_json.merge(day_od_total, how='left', left_on='prov_name', right_on = 'PROVINCIA')\n",
    "    gdf = geopandas.GeoDataFrame(\n",
    "    vis, geometry=vis['geometry']) # Conversione in GeoDataFrame\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1,figsize=(15, 15))\n",
    "    fig.suptitle('Ranking of provinces with most visitors by the total flux (map)',fontsize=20)\n",
    "    plt.axis('off')\n",
    "\n",
    "    gdf.plot(\n",
    "                 column='RATIO',\n",
    "                 ax=ax,\n",
    "                 cmap='OrRd',\n",
    "                 legend=True,\n",
    "                 legend_kwds={   'label': \"Logarithmic Flux\",\n",
    "                                 'orientation': 'vertical', 'shrink': 0.6},\n",
    "                 missing_kwds={  \"color\": \"white\",\n",
    "                                 \"edgecolor\": \"black\",\n",
    "                                 \"hatch\": \"\",\n",
    "                                 \"label\": \"Missing values\"},\n",
    "                 edgecolor='black',\n",
    "                 norm=colors.LogNorm(vmin=gdf.RATIO.min(), vmax=gdf.RATIO.max())#log scale,\n",
    "            )\n",
    "    \n",
    "    return\n",
    "\n",
    "graph_3_1()\n",
    "graph_3_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments\n",
    "\n",
    "### 1° Graph\n",
    "- The flux is scaled on the n° of habitants per province\n",
    "\n",
    "- Since this graph takes into account the total flux ratio and not montly or daily data, so here anomalies like 'Ascoli Piceno' province below aren't relevant\n",
    "\n",
    "- About 95% of the total flux is given by visitors living in Veneto\n",
    "\n",
    "- Not considering Veneto's provinces we still have that more than $50\\%$ is given by Trento, Ferrara, Udine.\n",
    "\n",
    "### 2° Graph\n",
    "- White provinces mean there is not data in the file, this means that the flux from those provinces is negligiable or not detected expect from 'Pordenone' where probably data had problems with the geometry column of the geojson dataframe.\n",
    "\n",
    "- As said before the flux is mainly focused on the Veneto region and in the nearby provinces\n",
    "\n",
    "- The flux is in logarithmic scale due to visualization needs\n",
    "\n",
    "- We see a non negligiable flux coming from Milano, Roma, Napoli and Torino even though they are located far from Padova and the data is rescaled to the number of inhabitants. This is probably due to the fact that there are trains from and to those regions very often in Padova.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_3_3():\n",
    "    \n",
    "    fig = go.Figure()\n",
    "\n",
    "    config = {'displayModeBar': True,}\n",
    "    data = day_od3\n",
    "\n",
    "    fig = px.bar(\n",
    "                 data[0:566],\n",
    "                 x=\"RATIO\", y=\"PROVINCIA\",\n",
    "                 color='DOW',\n",
    "                 barmode='stack',\n",
    "                 title=\"Ranking of provinces with most visitors montly\",\n",
    "                 category_orders={'MONTH':['Febbraio','Marzo','Aprile','Maggio'],\n",
    "                 'DOW':['Lunedì','Martedì','Mercoledì','Giovedì','Venerdì','Sabato','Domenica']},\n",
    "                 orientation='h',\n",
    "                 labels={\"RATIO\": \"\",\"PROVINCIA\": \"\"},\n",
    "                 hover_name=\"PROVINCIA\",\n",
    "                 log_x=True,\n",
    "                 color_discrete_sequence= px.colors.sequential.Agsunset,\n",
    "                 animation_frame=\"MONTH\"\n",
    "            )\n",
    "\n",
    "    fig.update_traces(textposition='inside')\n",
    "    \n",
    "    fig.update_layout(\n",
    "                  uniformtext_minsize=5,\n",
    "                  template='ggplot2',\n",
    "                  uniformtext_mode='show',\n",
    "                  xaxis_range=[-4.2,0.5],\n",
    "                  width=900,\n",
    "                  height=700,\n",
    "                  autosize=False\n",
    "                 )\n",
    "\n",
    "    fig.update_yaxes(\n",
    "                 categoryorder='total ascending',\n",
    "                 tickangle=-25,\n",
    "                 visible=True)\n",
    "\n",
    "    fig.update_xaxes(\n",
    "                 fixedrange=True,\n",
    "                 mirror='all',\n",
    "                 tickwidth=1,\n",
    "                 side='top')\n",
    "\n",
    "    fig.show(config=config)\n",
    "    \n",
    "    return\n",
    "\n",
    "graph_3_3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments\n",
    "\n",
    "\n",
    "### 3° Graph\n",
    "- The flux is scaled on the n° of habitants per province.\n",
    "\n",
    "- The graph has a logarithmic scale due to visualization needs.\n",
    "\n",
    "- Not all provinces have data for all the weekly or month period expecially in the lower part.\n",
    "\n",
    "- Data is sorted with ```categoryorder ='total ascending'```so that the rank reflects the total sum of the flux during the week. If there is the need for the rank of a particular day of the week to see the trend (e.g. in weekends) the flux should be rated as ```categoryorder = 'max ascending'``` in the code.\n",
    "\n",
    "- The first province is indeed Padova by orders of magnitude, then follow the nearby provinces as expected. Also, there's an important flux of visitors coming from Lombardia region (Milano, Mantova, Varese, Brescia, Bergamo, Monza)\n",
    "\n",
    "- It is visible a general progressive increase in the flux from February to April and the majority  of the provinces show a decrease in the weekend days as expected. There are some few exceptions as Milano that has a somewhat constant flux during the week but with a peak on saturdays as forx example Brescia (Part 4 will explain in detail).\n",
    "\n",
    "- Ascoli Piceno province does not have all data regarding months and days, despite this we see an unusual important flux with a peak on satudays of February (football match?). \n",
    "\n",
    "- The first 13 provinces ranking remain constant during all the months period (for example Trento province remains the first province outside Veneto in every month and even for every day), while for the rest there are some slight changes. The data has to be viewed as logarothmic so we actually excpect that the ranking will stay the same in the upper chart region while it could be subjected to some fluctuations in the lower part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4\n",
    "\n",
    "### Giorgetti Sabrina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1\n",
    "In this first part, to study the visitors's fluxes from the nearby regions only, we've selected one region for each of the different directions toward Padova:\n",
    "\n",
    "- Lombardia at west, associated with the A4 toward Milano-Torino\n",
    "- Emilia Romagna at south, associated with the A13 toward Bologna-Roma\n",
    "- Friuli Venezia Giulia at east, associated with the A4 toward Venice-Trieste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA FOR THE REGIONS OF INTEREST\n",
    "#For the regions of interest insert the region capital names and the region names\n",
    "cap=['Milano', 'Bologna', 'Trieste',]\n",
    "reg=['Lombardia', 'Emilia Romagna', 'Friuli Venezia Giulia']\n",
    "\n",
    "codici_istat_provincia=codist_prov\n",
    "data_day_od=day_od\n",
    "\n",
    "#Function to obtain the info of the regions of interest \n",
    "def info_regions(codici_istat_provincia, capoluogo):\n",
    "    cod_reg=codici_istat_provincia[codici_istat_provincia['PROVINCIA'] == capoluogo]['COD_REG'].values[0] \n",
    "    info_reg=codici_istat_provincia[codici_istat_provincia['COD_REG'] == cod_reg]\n",
    "    return info_reg\n",
    "\n",
    "#Create a dictionary of dataframes for the different regions (cod info only)\n",
    "d = {} \n",
    "cod_int=[]\n",
    "for c in cap:\n",
    "    d[reg[cap.index(c)]] = pd.DataFrame(info_regions(codici_istat_provincia,c))\n",
    "    #dataframe d[key] with key=reg[i], data: |COD_REG|COD_PRO|PROVINCIA|PROV_SIGLA\n",
    "    \n",
    "\n",
    "#Returns a dataset with all the info (data_day_od) of the regions of interest\n",
    "def data_regions(data_day_od, cod_all_prov):\n",
    "    data_reg=data_day_od[data_day_od['COD_PRO'].isin(cod_all_prov)]\n",
    "    return data_reg\n",
    "\n",
    "#Define a dictionary with the dataframe for the different regions with all the info from data_day_od\n",
    "#data_reg[reg[i]]  MONTH|DOW|ORIGIN|DESTINATION|CUST_CLASS|COD_CONTRY|COD_PRO|PRO_COM|FLOW\n",
    "data_reg = {} \n",
    "for i in range(len(reg)):\n",
    "    data_reg[reg[i]] = pd.DataFrame(data_regions(data_day_od,d[reg[i]]['COD_PRO'].values[:]))\n",
    "    \n",
    "\n",
    "##STUDY OF THE FLUXES OF NEARBY REGIONS TO FIND WHICH OF THE THREE DIRECTIONS HAS TO BE PRIORITIZE\n",
    "\n",
    "#Function to calculate the total flow of the regions of interest \n",
    "def flow_regions(info_data, cod_all_prov):\n",
    "    flow_sum=info_data.groupby(info_data['COD_PRO'].isin(cod_all_prov))['FLOW'].sum()\n",
    "    flow_sum=flow_sum.reset_index()\n",
    "    return flow_sum['FLOW'][flow_sum['COD_PRO'] == True][0]\n",
    "    \n",
    "#Create a dictionary for the the total flux for the different regions \n",
    "d_flow = {}\n",
    "for i in range(len(reg)):\n",
    "    d_flow[reg[i]] = flow_regions(data_reg[reg[i]], d[reg[i]]['COD_PRO'].values[:])\n",
    "    \n",
    "    \n",
    "#Dataframe with the total flux for the regions of interest, flow: REGION|FLOW|\n",
    "flow= pd.DataFrame(d_flow.items(), columns=['REGION', 'FLOW']).sort_values(by='FLOW', ascending=False)\n",
    "\n",
    "\n",
    "#Display the results\n",
    "table_color =  [[0, '#004d00'],[.5, '#ffffff'],[1, '#ffffff']]\n",
    "fig = ff.create_table(flow, height_constant=80, colorscale=table_color) # font_colors=font)#colorscale\n",
    "\n",
    "color_dict = ['#b32d00', '#fb9804' ,'#b5de2b']\n",
    "fig.add_trace(go.Bar(x = flow[\"REGION\"], y = flow[\"FLOW\"],marker_color=color_dict,name='Flow',xaxis='x2', yaxis='y2',))\n",
    "    \n",
    "fig.update_layout(title_text = 'Total flow for the nearby regions',margin = {'t':60, 'b':80, },\n",
    "    xaxis = {'domain': [0, .4]},\n",
    "    xaxis2 = {'domain': [0.5, 1.], 'title':'Region'},\n",
    "    yaxis2 = {'domain': [0, 1], 'anchor': 'x2', 'title': 'Flow'}\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from this plot the region with the highest flux is \"Lombardia\", followed by \"Emilia Romagna\" and \"Friuli Venezia Giulia\". Of the three directions it should be therefore proritized the Italian highways A4, torward Milano-Torino.\n",
    "\n",
    "# 4.2\n",
    "Let's performe a more detailed analysis selecting only the most contributing provinces for each of the selected regions. We'll first study the contribute to the flow of each province and then study the flow trend with respect to the days of the week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2.1 STUDY OF THE FLUXES OF THE MOSTLY CONTRIBUTING PROVINCES BASED ON THE DAY OF THE WEEK \n",
    "\n",
    "#SELECTION OF THE PROVINCES THAT ARE MOSTLY CONTRIBUTING BASED ON THE FLOW \n",
    "\n",
    "#Function to find the mostly contributing provinces for each region:\n",
    "#Returns the total flux for each province and the code of the selected provinces\n",
    "def flow_province(data_regions):\n",
    "    flow_province=data_regions.groupby('COD_PRO').FLOW.sum()\n",
    "    flow_sorted=flow_province.sort_values(ascending=False)\n",
    "    flow_selected=flow_sorted.head(n=3).reset_index()  #Selected the first 3 with the highest flux\n",
    "    cod_pro_selected=flow_selected['COD_PRO'].values[:]\n",
    "    return flow_selected, cod_pro_selected\n",
    "\n",
    "\n",
    "flow_pro={} #Dictionary with a flow dataframe for the different regions\n",
    "pro_selected={}  #Dictionary with the code of the selected provinces\n",
    "\n",
    "for i in range(len(reg)): \n",
    "    flow_pro[reg[i]],  pro_selected[reg[i]] = flow_province(data_reg[reg[i]]) \n",
    "    \n",
    "\n",
    "#Associating to each province code the province name\n",
    "#flow_pro: flow for the provinces of a region flow_pro[reg[i]]: 'COD PRO', 'NAME', 'FLOW'\n",
    "for i in range(len(reg)):\n",
    "    name_l=[]\n",
    "    for cod in flow_pro[reg[i]]['COD_PRO'].values[:]:\n",
    "        if cod in d[reg[i]]['COD_PRO'].values[:]:\n",
    "            where=np.argwhere(d[reg[i]]['COD_PRO'].values[:]==cod)\n",
    "            idx=where[0][0] \n",
    "            name=d[reg[i]]['PROVINCIA'].values[idx:idx+1][0]\n",
    "            name_l.append(name)\n",
    "    flow_pro[reg[i]].insert(1, \"NAME\", name_l)\n",
    "\n",
    "    \n",
    "#Display the results\n",
    "table_color =  [[0, '#004d00'],[.5, '#ffffff'],[1, '#ffffff']]\n",
    "color_dict = ['#b32d00', '#fb9804' ,'#b5de2b']\n",
    "\n",
    "\n",
    "#Plot flow for the provinces\n",
    "fig_fp = go.Figure()\n",
    "for i in range(len(reg)):  \n",
    "        fig_fp.add_trace(go.Bar(x = flow_pro[reg[i]][\"NAME\"], y = flow_pro[reg[i]][\"FLOW\"], name=reg[i], marker_color=color_dict[i]))\n",
    "        fig_fp.update_layout(barmode='relative')\n",
    "        fig_fp.update_layout(title={'text': \"Flow for the mostly contributing provinces\",'y':0.9,'x':0.5,'xanchor': 'center','yanchor': 'top'},xaxis = {'domain': [0.1, 1.], 'title':'Province'}, yaxis = {'domain': [0, 1], 'anchor': 'x2', 'title': 'Flow'})\n",
    "\n",
    "fig_fp.show()\n",
    "\n",
    "#Dataframe\n",
    "for i in range(len(reg)):  \n",
    "    fig = ff.create_table(flow_pro[reg[i]],colorscale=table_color)\n",
    "    fig.update_layout(xaxis = {'domain': [.2, .7]}, yaxis1 = {'domain': [0, 1]})\n",
    "    fig.update_layout(title={'text': str(reg[i]),'xanchor': 'left','yanchor': 'top'}, xaxis1 = {'domain': [0.3, .7]}, yaxis1 = {'domain': [0., 1]})\n",
    "    fig.show()\n",
    "     \n",
    "    \n",
    "#Display the new total flow\n",
    "tot_flow=[]\n",
    "for i in range(len(reg)):\n",
    "    tot_flow.append(flow_pro[reg[i]].FLOW.sum())\n",
    "\n",
    "new_tot_flow=go.Figure()\n",
    "new_tot_flow.add_trace(go.Bar(x =reg, y=tot_flow, marker_color=color_dict, name='Flow'))\n",
    "new_tot_flow.update_layout(title={'text': \"Total flow of the regions with only the mostrly contributing provinces\",'y':0.8,'x':0.5,'xanchor': 'center','yanchor': 'top'},\n",
    "                                   xaxis = {'domain': [0.1, 1.], 'title':'Region'}, \n",
    "                                   yaxis = {'domain': [0, 1], 'anchor': 'x2', 'title': 'Flow'},\n",
    "                                    margin=dict(l=130, r=160, t=150, b=100))\n",
    "\n",
    "new_tot_flow.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " From the first plot of the flow for the mostly contributing provinces we can see that the reason why Lombardia has such a high flux is connected to the province of Milan. Emilia Romagna and Friulia Venezia Giulia on the other hand ave a similar flux, with Bologna and Udine as the most contributing province for the region. Let's also notice that selecting only three provinces for region returns a ittle higher flux for Friuli Venezia Giulia instead of Emilia Romagna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.2.2 STUDY OF THE FLUXES BASED ON THE DAY OF THE WEEK\n",
    "\n",
    "#Returns a dataframe with all the info (from data_day_od) for the selected provinces for each region\n",
    "def data_selected_province(data_day_od,cod_pro_selected):\n",
    "    data_province_selected=data_day_od[data_day_od['COD_PRO'].isin(cod_pro_selected)] \n",
    "    return data_province_selected\n",
    "\n",
    "#Define a dictionary with a dataframe for the different regions, containg all the info of the selected provinces \n",
    "d_selected_province = {} \n",
    "for i in range(len(reg)):\n",
    "    d_selected_province[reg[i]] = pd.DataFrame(data_selected_province(data_day_od,flow_pro[reg[i]]['COD_PRO'].values[:]))\n",
    "    \n",
    "#Sum of the fluxes based on the day of the week and the province\n",
    "def flow_days(data_province_selected, cod_pro_selected):\n",
    "    flow_days=data_province_selected.groupby(['DOW', 'COD_PRO']).FLOW.sum().reset_index()\n",
    "    return flow_days\n",
    "\n",
    "#Dictrionary with a dataframe for each region, with days information\n",
    "d_flow_days = {} \n",
    "for i in range(len(reg)):\n",
    "    d_flow_days[reg[i]] = pd.DataFrame(flow_days(d_selected_province[reg[i]],flow_pro[reg[i]]['COD_PRO']))\n",
    "\n",
    "#Printing the dataframe associating the name to each province code\n",
    "for i in range(len(reg)):\n",
    "    name_list=[]\n",
    "    for cod in d_flow_days[reg[i]]['COD_PRO'].values[:]:\n",
    "        if cod in d[reg[i]]['COD_PRO'].values[:]:\n",
    "            where=np.argwhere(d[reg[i]]['COD_PRO'].values[:]==cod)\n",
    "            idx=where[0][0] #torna poco questo 00\n",
    "            name=d[reg[i]]['PROVINCIA'].values[idx:idx+1][0]\n",
    "            name_list.append(name)  \n",
    "    d_flow_days[reg[i]].insert(1, \"NAME\", name_list)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "#Analysis based on the days\n",
    "days = ['Lunedì', 'Martedì', 'Mercoledì', 'Giovedì', 'Venerdì', 'Sabato', 'Domenica']\n",
    "\n",
    "#Function to order the dataset based on the days of the week \n",
    "def days_order(dataset,days):\n",
    "    dataset['DOW']=pd.Categorical(dataset['DOW'], categories=days, ordered=True)\n",
    "    dataset_days=dataset.sort_values(['DOW'], ignore_index=True)\n",
    "    return dataset_days\n",
    "\n",
    "#Function to sort the data by provinces\n",
    "def sort(dataset):\n",
    "    dataset=dataset.sort_values(['COD_PRO', 'NAME'],ascending=False, ignore_index=True)\n",
    "    return dataset\n",
    "\n",
    "#Dictionary with a dataframe for each region\n",
    "#Different dataframes based on different type of data organization\n",
    "flow_each_pro={}\n",
    "\n",
    "for i in range(len(reg)):\n",
    "    list_k=[0]\n",
    "    flow_each_pro[reg[i]]=sort(d_flow_days[reg[i]])\n",
    "    for k in range(1,len(flow_each_pro[reg[i]]['COD_PRO'])):\n",
    "        if flow_each_pro[reg[i]]['COD_PRO'][k]!=flow_each_pro[reg[i]]['COD_PRO'][k-1]:\n",
    "            list_k.append(k)\n",
    "    list_k.append(len(flow_each_pro[reg[i]]['COD_PRO']))\n",
    "    for d in range(len(list_k)-1): \n",
    "            flow_each_pro[reg[i],d]= pd.DataFrame(days_order(flow_each_pro[reg[i]][list_k[d]:list_k[d+1]],days))\n",
    "    flow_each_pro[reg[i]]=pd.DataFrame(days_order(d_flow_days[reg[i]],days))\n",
    "    \n",
    "    \n",
    "#Funtion to sum the fluxes of a day for each region \n",
    "def flux_sum_day(flow_each_pro):\n",
    "    flux_sum_pd=flow_each_pro.groupby(['DOW'])['FLOW'].sum().reset_index()\n",
    "    return flux_sum_pd\n",
    "\n",
    "#Dictionary with dataframes for each region with the sum of fluxes based on the day of the week\n",
    "flux_sum_perday={}\n",
    "for i in range(len(reg)):\n",
    "        flux_sum_perday[reg[i]]=flux_sum_day(flow_each_pro[reg[i]])\n",
    "        \n",
    "\n",
    "#DIsplay the results\n",
    "color_dict = ['#b32d00', '#fb9804' ,'#b5de2b']\n",
    "table_color =  [[0, '#004d00'],[.5, '#ffffff'],[1, '#ffffff']]   \n",
    "\n",
    "\n",
    "#Results for the single regions based on the days of the week\n",
    "#Plot\n",
    "fig_regdf = go.Figure()\n",
    "for i in range(len(reg)):  \n",
    "        fig_regdf = fig_regdf.add_trace(go.Scatter(x = flux_sum_perday[reg[i]][\"DOW\"], y = flux_sum_perday[reg[i]][\"FLOW\"],name =reg[i], marker=dict(color=color_dict[i],size=8,line=dict(color=color_dict[i],width=8))))\n",
    "        fig_regdf.update_layout(title={'text': \"Flow for the selected regions based on the days of the week\",'y':0.9,'x':0.5,'xanchor': 'center','yanchor': 'top'},xaxis = {'domain': [0.1, 1.], 'title':'Day of the week'}, yaxis = {'domain': [0, 1], 'anchor': 'x2', 'title': 'Flow'})\n",
    "fig_regdf.show()\n",
    "\n",
    "\n",
    "##Print dataset if needed\n",
    "#for i in range(len(reg)):  \n",
    "#    fig_tab_regdf = ff.create_table(flux_sum_perday[reg[i]],colorscale=table_color)\n",
    "#    fig_tab_regdf.update_layout(title={'text': str(reg[i]),'xanchor': 'left','yanchor': 'top'}, xaxis1 = {'domain': [0.3, .8]}, yaxis1 = {'domain': [0., 1]})\n",
    "#    fig_tab_regdf.show()\n",
    "#    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "#Results for each province based on the days of the week\n",
    "#Plot\n",
    "fig_prodf = go.Figure()\n",
    "color_dict = [ ['#e61100','#ff1a1a','#ff9f80'], ['#ffff66','#ff8000', '#ffd480'], [ '#b5de2b','#4dff4d','#00cc00' ]]\n",
    "\n",
    "for i in range(len(reg)):  \n",
    "    for k in range(len(flow_pro[reg[i]]['COD_PRO'])):\n",
    "        fig_prodf = fig_prodf.add_trace(go.Bar(x = flow_each_pro[reg[i],k][\"DOW\"], y = flow_each_pro[reg[i],k][\"FLOW\"], name = flow_each_pro[reg[i],k].iloc[0,1], marker_color=color_dict[i][2-k]))\n",
    "        fig_prodf.update_layout(title={'text': \"Flow for the mostly contributing provinces based on the days of the week\",'y':0.9,'x':0.5,'xanchor': 'center','yanchor': 'top'},xaxis = {'domain': [0.1, 1.], 'title':'Day of the week'}, yaxis = {'domain': [0, 1], 'anchor': 'x2', 'title': 'Flow'})\n",
    "fig_prodf.show()\n",
    "\n",
    "\n",
    "##Print dataset if needed    \n",
    "#for i in range(len(reg)):  \n",
    "#    fig_tab_prodf = ff.create_table(flow_each_pro[reg[i]],colorscale=table_color, name=reg[i])\n",
    "#    fig_tab_prodf.update_layout(title={'text': str(reg[i]),'xanchor': 'left','yanchor': 'top'}, xaxis1 = {'domain': [0.3, .8]}, yaxis1 = {'domain': [0., 1]})\n",
    "#    fig_tab_prodf.show()\n",
    "#    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In the first plot we can see the trend for the three regions. For Lombardia we have an increasing flow going into the weekends with a peak on Saturday. Friuli Venezia Giulia has instead an opposite trend with a higher flux during the week days and a drop on Saturday and Sunday. For Emilia Romagna we have quite a constant flow with a peaks on Wednesday. Looking at total flow we can see a lower value on the weekends, suggesting a mobility linked to commuters during the weekdays.\n",
    "\n",
    "In the second plot we can see that Milan has the highest flow, at least two times bigger than the others provinces. The trend for Milano explains the Lombardia’s trend while Brescia and Mantova have an opposite trend with a lower flow on the weekends As regards Emilia Romagna, the trend for Bologna is in agreement with the one of the entire region, while for Ferrara the flux gets smaller on the weekends.For Friuli Venezia Gulia the contribution to the flow is mostly given by Pordenone and Udine qith a higher flow during the week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3\n",
    "In this part we want to see what happens if we'll consider more provinces that are present in one of the three directions. We are also interested to see what happens if we consider provinces that belongs to the region of Veneto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#For the regions of interest insert the region capital names and the region names\n",
    "m_cap=['Milano', 'Bologna', 'Trieste', 'Roma', 'Torino', 'Firenze', 'Napoli','Trento','Venezia']\n",
    "m_reg=['Lombardia', 'Emilia Romagna', 'Friuli Venezia Giulia', 'Lazio', 'Piemonte', 'Toscana', 'Campania','Trentino Alto Adige','Veneto']\n",
    "\n",
    "#Dataframes for the different regions (cod info only)\n",
    "m_d = {} \n",
    "for c in m_cap:\n",
    "    m_d[m_reg[m_cap.index(c)]] = pd.DataFrame(info_regions(codici_istat_provincia,c))\n",
    "\n",
    "#Dataframes for the different regions with all the info from data_day_od\n",
    "m_data_reg = {} \n",
    "for i in range(len(m_reg)):\n",
    "    m_data_reg[m_reg[i]] = pd.DataFrame(data_regions(data_day_od,m_d[m_reg[i]]['COD_PRO'].values[:]))\n",
    "    \n",
    "code_padua=codici_istat_provincia[codici_istat_provincia['PROVINCIA']=='Padova']['COD_PRO'].values[0]\n",
    "\n",
    "\n",
    "#STUDY based on the provinces and days of the week (as 4.2)\n",
    "\n",
    "#Function to find the mostly contributing provinces for each region\n",
    "#Returns the total flux for each province and the code of the selected provinces\n",
    "#Padua will not be considered \n",
    "def flow_province(data_regions):\n",
    "    flow_province=data_regions.groupby('COD_PRO').FLOW.sum()\n",
    "    flow_sorted=flow_province.sort_values(ascending=False)\n",
    "    flow_selected=flow_sorted.loc[flow_sorted[:] >= 1e4].reset_index() #Selection based on the flux\n",
    "    for cod in flow_sorted.reset_index()['COD_PRO'].values[:]:\n",
    "        if cod == code_padua:\n",
    "            flow_selected=flow_sorted.loc[flow_sorted[:] >= 1e4].reset_index()\n",
    "            flow_selected=flow_selected.loc[1:] #not considering Padua\n",
    "    cod_pro_selected=flow_selected['COD_PRO'].values[:]\n",
    "    return flow_selected, cod_pro_selected\n",
    "\n",
    "\n",
    "m_flow_pro={} \n",
    "m_pro_selected={} \n",
    "\n",
    "for i in range(len(m_reg)): \n",
    "    m_flow_pro[m_reg[i]], m_pro_selected[m_reg[i]] = flow_province(m_data_reg[m_reg[i]]) \n",
    "\n",
    "\n",
    "#Associating each province code the province name\n",
    "#flow of the province for a region flow_pro[reg[i]]: 'COD PRO', 'NAME', 'FLOW'\n",
    "for i in range(len(m_reg)):\n",
    "    name_l=[]\n",
    "    for cod in m_flow_pro[m_reg[i]]['COD_PRO'].values[:]:\n",
    "        if cod in m_d[m_reg[i]]['COD_PRO'].values[:]:\n",
    "            where=np.argwhere(m_d[m_reg[i]]['COD_PRO'].values[:]==cod)\n",
    "            idx=where[0][0] \n",
    "            name=m_d[m_reg[i]]['PROVINCIA'].values[idx:idx+1][0]\n",
    "            name_l.append(name)\n",
    "    m_flow_pro[m_reg[i]].insert(1, \"NAME\", name_l)\n",
    "     \n",
    "    \n",
    "#Display the results\n",
    "table_color =  [[0, '#004d00'],[.5, '#ffffff'],[1, '#ffffff']]\n",
    "color_dict=['#b32d00', '#fb9804' ,'#b5de2b','#6ece8c', '#9ce2bf','#94c0aa','#39604d','#2277a4', 'blue']\n",
    "\n",
    "#Plot no Veneto\n",
    "fig_morep = go.Figure()\n",
    "for i in range(len(m_reg)-1):  \n",
    "        fig_morep.add_trace(go.Bar(x = m_flow_pro[m_reg[i]][\"NAME\"], y = m_flow_pro[m_reg[i]][\"FLOW\"], name=m_reg[i], marker_color=color_dict[i]))\n",
    "        fig_morep.update_layout(barmode='relative')\n",
    "        fig_morep.update_layout(title={'text': \"Flow for the selected provinces\",'y':0.9,'x':0.45,'xanchor': 'center','yanchor': 'top'},\n",
    "                                xaxis = {'domain': [0.1, 1.], 'title':'Province'}, \n",
    "                                yaxis = {'domain': [0, 1], 'anchor': 'x2', 'title': 'Flow'},\n",
    "                                margin=dict(l=80, r=180, t=90, b=90))\n",
    "fig_morep.show()\n",
    "\n",
    "\n",
    "#Highways no Veneto\n",
    "more_prov=pd.concat(m_flow_pro.values(), axis=0)\n",
    "more_prov=more_prov.reset_index()[:-6]\n",
    "highway=['A4-west','A4-west','A4-west','A4-west','A13','A13','A4-east','A4-east','A13','A4-west','A13','A13','A4-west','A4-west']\n",
    "more_prov.insert(3,'DIR',highway)  \n",
    "flow_dir=more_prov.groupby(by='DIR').FLOW.sum().reset_index().sort_values(by='FLOW',ascending=False)\n",
    "\n",
    "fig_flow_dir = ff.create_table(flow_dir, height_constant=80, colorscale=table_color) # font_colors=font)#colorscale\n",
    "\n",
    "color_dict_a = ['#b32d00', '#fb9804' ,'#b5de2b']\n",
    "fig_flow_dir.add_trace(go.Bar(x = flow_dir[\"DIR\"], y = flow_dir[\"FLOW\"], marker_color=color_dict_a,name='Flow',xaxis='x2', yaxis='y2',))\n",
    "    \n",
    "fig_flow_dir.update_layout(title_text = 'Total flow for the three highways',margin = {'t':60, 'b':80, },\n",
    "    xaxis = {'domain': [0, .4]},\n",
    "    xaxis2 = {'domain': [0.5, 1.], 'title':'Highway'},\n",
    "    yaxis2 = {'domain': [0, 1], 'anchor': 'x2', 'title': 'Flow'}\n",
    ")\n",
    "\n",
    "fig_flow_dir.show()\n",
    "\n",
    "\n",
    "#Plot with Veneto\n",
    "fig_allp=go.Figure()\n",
    "for i in range(len(m_reg)):  \n",
    "        fig_allp.add_trace(go.Bar(x = m_flow_pro[m_reg[i]][\"NAME\"], y = m_flow_pro[m_reg[i]][\"FLOW\"], name=m_reg[i], marker_color=color_dict[i]))\n",
    "        fig_allp.update_layout(title={'text': \"Flow including provinces of Veneto\",'y':0.9,'x':0.45,'xanchor': 'center','yanchor': 'top'},\n",
    "                                xaxis = {'domain': [0.1, 1.], 'title':'Province'}, \n",
    "                                yaxis = {'domain': [0, 1], 'anchor': 'x2', 'title': 'Flow'},\n",
    "                                margin=dict(l=80, r=180, t=90, b=90))\n",
    "fig_allp.show()\n",
    "\n",
    "\n",
    "#Highways with Veneto\n",
    "all_prov=pd.concat(m_flow_pro.values(), axis=0)\n",
    "highway=['A4-west','A4-west','A4-west','A4-west','A13','A13','A4-east','A4-east','A13','A4-west','A13','A13','A4-west','A4-west',\n",
    "         'A4-east','A4-west','A4-east','A4-west','A13','A4-east']\n",
    "all_prov.insert(3,'DIR',highway)  \n",
    "all_flow_dir=all_prov.groupby(by='DIR').FLOW.sum().reset_index().sort_values(by='FLOW',ascending=False)\n",
    "\n",
    "table_color =  [[0, '#004d00'],[.5, '#ffffff'],[1, '#ffffff']]\n",
    "fig_all_flow_dir = ff.create_table(all_flow_dir, height_constant=80, colorscale=table_color) # font_colors=font)#colorscale\n",
    "\n",
    "color_dict_a = ['#b5de2b','#b32d00', '#fb9804']\n",
    "fig_all_flow_dir.add_trace(go.Bar(x = all_flow_dir[\"DIR\"], y = all_flow_dir[\"FLOW\"], marker_color=color_dict_a,name='Flow',xaxis='x2', yaxis='y2',))\n",
    "    \n",
    "fig_all_flow_dir.update_layout(title_text = 'Total flow for the three highways with Veneto',margin = {'t':60, 'b':80, },\n",
    "    xaxis = {'domain': [0, .4]},\n",
    "    xaxis2 = {'domain': [0.5, 1.], 'title':'Highway'},\n",
    "    yaxis2 = {'domain': [0, 1], 'anchor': 'x2', 'title': 'Flow'}\n",
    ")\n",
    "\n",
    "fig_all_flow_dir.show()\n",
    "\n",
    "    \n",
    "#Dataframe with all the info\n",
    "print(\"\\n\")\n",
    "tab_flow_dir_prov= ff.create_table(all_prov,colorscale=table_color)\n",
    "tab_flow_dir_prov.update_layout(xaxis = {'domain': [.2, .7]}, yaxis1 = {'domain': [0, 1]})\n",
    "tab_flow_dir_prov.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first case we take into consideration also Torino, Trento and Bolzano on the A4-west highway and Firenze, Roma and Napoli for the A13. From the first plot we can see that Roma and Trento are the two new provinces contributing the most, after Milan. Considering then the total flow for the three highways ,it results, once again, that the A4-west should be prioritized being the one with the highest flow.\n",
    "\n",
    "However, in the second case, we observe that the biggest contribution to the flow is given by the provinces of Veneto, in particular Venezia. From this it follows that the highway with the highest flow is no longer the A4-west but the A4-east, so the one to be prioritized should be the highway section connecting Padova to Venezia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5\n",
    "\n",
    "### Avella Michele "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data\n",
    "distinct_users_day.csv contains the information about the visitors and their origin.\n",
    "\n",
    "codici_istat_provincia.csv contains the name and the code of every Italian 'provincie'.\n",
    "\n",
    "Veneto.txt is a matrix with the distances beteen all the Italian 'comuni'.\n",
    "\n",
    "codici_istat_comune.csv contains the name and the code of every Italian's 'comune' and the code of its 'provincia'.\n",
    "\n",
    "We want to compute the distribution of the visitors as a function of the distance of their origin 'provincia' with respect to Padova.\n",
    "\n",
    "In this firts section we will compute the distances from all the italian 'provinice' and Padova. Than we will build a dataframe containig all the visitors from all the 'provincie'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next two blocks we replace the COD_COM with the COD_PRO in the distances matrix. The problem here is that codici_istat_comune does not have all the COMUNI. We avoid this problem by replacing the missing COD_COM with the next known COD_COM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic={}\n",
    "for index, row in codist_com.iterrows():\n",
    "    dic[row.PRO_COM]=row.COD_PRO\n",
    "\n",
    "def com_to_prov(c):\n",
    "    try:\n",
    "        p=dic[c]\n",
    "    except:\n",
    "        p=com_to_prov(c+1)\n",
    "    return p\n",
    "com_to_prov=np.vectorize(com_to_prov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origine=codist_ven.Origine.astype(int).values\n",
    "destinazione=codist_ven.Destinazione.astype(int).values\n",
    "\n",
    "\n",
    "\n",
    "print(origine[:2])\n",
    "origine=com_to_prov(origine)\n",
    "print(origine[:2])\n",
    "\n",
    "destinazione=com_to_prov(destinazione)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cod_Padova=codist_prov[codist_prov.PROVINCIA=='Padova'].COD_PRO.values[0]\n",
    "codist_ven['Origine']=origine\n",
    "codist_ven['Destinazione']=destinazione\n",
    "\n",
    "codist_ven=codist_ven[codist_ven.Destinazione==cod_Padova]\n",
    "codist_ven=codist_ven.groupby(['Origine','Destinazione'],as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_users_day=dist_users_day.groupby(['COD_PRO'],as_index=False).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same problem as before: some COD_PRO are missing. In this case we just remove the missing COD_PRO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic={}\n",
    "for index, row in codist_ven.iterrows():\n",
    "    dic[row.Origine]=row.Total_Mete\n",
    "\n",
    "def prov_to_dist(p):\n",
    "    try: \n",
    "        d=dic[p]\n",
    "    except:\n",
    "        d=-1\n",
    "    return d \n",
    "prov_to_dist=np.vectorize(prov_to_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_users_day['DISTANCES']=prov_to_dist(dist_users_day.COD_PRO.astype(int))\n",
    "print(np.sum(dist_users_day.VISITORS[dist_users_day.DISTANCES==-1]))\n",
    "dist_users_day=dist_users_day[dist_users_day.DISTANCES!=-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_users_day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the distribution\n",
    "Now we can compute the distribtion. We will do it for different number of bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def distribution(N,df):\n",
    "    dist=df.DISTANCES.values\n",
    "    freq=df.VISITORS.values\n",
    "\n",
    "    bins=np.linspace(np.min(dist),np.max(dist),N+1)\n",
    "\n",
    "    f_bins=[]\n",
    "    for i in range(len(bins)-1):\n",
    "        f=np.sum(freq[dist<=bins[i+1]])\n",
    "        f-=np.sum(freq[dist<bins[i]])\n",
    "        f_bins.append(f)\n",
    "\n",
    "    bins=(bins[1:]+bins[:-1])*0.5\n",
    "\n",
    "    return bins, f_bins\n",
    "\n",
    "nbins=100\n",
    "\n",
    "bins, f_bins = distribution(nbins,dist_users_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=bins, y=f_bins, name='Distribution',\n",
    "                        mode='lines+markers',\n",
    "                        line=dict(color='royalblue', width=2, dash='dot')\n",
    "                         ))\n",
    "\n",
    "fig.update_layout(title=\"Distribution of visitors as a function of distance from Padua \",\n",
    "                xaxis_title=\"Distance [m]\",\n",
    "                yaxis_title=\"Visitors\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " As we can observe in the plot above, our distribution present lot of peaks. These peaks corresponds to the most popolous italian provincies as we can see in the next plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_users_day=dist_users_day[dist_users_day.COD_PRO!=28] #REMOVING PADOVA\n",
    "nbins=180\n",
    "bins, f_bins = distribution(nbins,dist_users_day)\n",
    "\n",
    "big=['Milano','Venezia','Roma','Napoli','Bologna','Torino','Verona','Brescia','Firenze','Genova','Varese']\n",
    "cod_big=[]\n",
    "for i in big:\n",
    "    cod_big.append(codist_prov[codist_prov.PROVINCIA==i].COD_PRO.values[0])\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "\n",
    "for i in range(len(big)):\n",
    "    fig.add_trace(go.Scatter(x=[prov_to_dist(cod_big[i]),prov_to_dist(cod_big[i])], \n",
    "                            y=[min(f_bins),max(f_bins)],\n",
    "                            mode='lines+text',\n",
    "                            showlegend=False,\n",
    "                            line=dict(color='red', width=2, dash='dot'),\n",
    "                            opacity=.5,\n",
    "                         ))\n",
    "\n",
    "    fig.add_annotation(x=prov_to_dist(cod_big[i])+9e3, y=0.5*(min(f_bins)+max(f_bins)),\n",
    "            text=big[i],\n",
    "            showarrow=False,\n",
    "            textangle=-90,\n",
    "            font=dict(color='red',size=15))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=bins, y=f_bins, name='Distribution',\n",
    "                        mode='lines+markers',\n",
    "                        line=dict(color='royalblue', width=2, dash='dot'),\n",
    "                        showlegend=False,\n",
    "                        opacity=.7\n",
    "                         ))\n",
    "\n",
    "\n",
    "\n",
    "fig.update_xaxes(range=[0, 0.75*1e6])\n",
    "fig.update_layout(title=\"Distribution of visitor as a function of distance from Padua \",\n",
    "                xaxis_title=\"Distance [m]\",\n",
    "                yaxis_title=\"Visitors\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df=pd.DataFrame({})\n",
    "b_df=np.array([])\n",
    "x_df=np.array([])\n",
    "y_df=np.array([])\n",
    "for b in range(30,200,10):\n",
    "    bins, f_bins = distribution(b,dist_users_day)\n",
    "    f_bins=np.array(f_bins)\n",
    "\n",
    "    x_df=np.concatenate((x_df,bins))\n",
    "    y_df=np.concatenate((y_df,f_bins))\n",
    "\n",
    "    b_df=np.concatenate((b_df,b*np.ones(len(bins))))\n",
    "\n",
    "df['x']=x_df\n",
    "df['y']=y_df\n",
    "df['bins']=b_df.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.line(df, x=\"x\", y=\"y\", animation_frame=\"bins\",\n",
    "                range_x=[min(df.x)-3e3,0.75*1e6],\n",
    "                title=\"Distribution of visitor as a function of distance from Padua \")\n",
    "fig.update_layout(title=\"Distribution of visitors as a function of distance from Padua \",\n",
    "                xaxis_title=\"Distance [m]\",\n",
    "                yaxis_title=\"Visitors\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In the last plot we can observe that the distribution's shape changes with the number of bins. As we increase the number of bins also increase the number of peaks. This beahaviour is due to the non-uniform distribution of the italian poulation that is more concentrated in the big cities.\n",
    "\n",
    "In order to make a regression we will use a hight number of bins = 100 as the distribution's shape is quite stable after that value. Since the behavior is decreasing we will use a function : \n",
    "$f(d)=\\frac{A}{(d-B)^2} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "def f(x,A,B):\n",
    "    return A/((x-B)**2)\n",
    "\n",
    "nbins=130\n",
    "bins, f_bins = distribution(nbins,dist_users_day)\n",
    "bins/=1e6\n",
    "f_bins=np.array(f_bins)/1e6\n",
    "\n",
    "max_params, params_covariance = optimize.curve_fit(f, bins, f_bins, p0=[0.03, -0.03])\n",
    "x=np.linspace(np.min(bins),np.max(bins),200)\n",
    "\n",
    "sigma_ab=np.square(np.diag(params_covariance))\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x, y=f(x,max_params[0],max_params[1]), name='Fit',\n",
    "                        mode='lines',\n",
    "                        line=dict(color='orange', width=2, dash='dot'),\n",
    "                        opacity=0.8\n",
    "                         ))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=bins, y=f_bins, name='Distribution',\n",
    "                        mode='markers',\n",
    "                        marker=dict(color='blue',size=6),\n",
    "                        opacity=0.5\n",
    "                         ))\n",
    "\n",
    "fig.update_layout(title=\"Fit of the previous distribution (nbins= \"+str(nbins)+\") \",\n",
    "                xaxis_title=\"Distance [1e6 m]\",\n",
    "                yaxis_title=\"Visitors 1e6\")\n",
    "fig.show()\n",
    "print(sigma_ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins=200\n",
    "bins, f_bins = distribution(nbins,dist_users_day)\n",
    "bins/=1e6\n",
    "f_bins=np.array(f_bins)/1e6\n",
    "\n",
    "x=np.linspace(np.min(bins),np.max(bins),200)\n",
    "\n",
    "sigma_ab=np.square(np.diag(params_covariance))\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "\n",
    "\n",
    "fig.add_trace(go.Scatter(x=bins, y=f_bins, name='Distribution',\n",
    "                        mode='markers',\n",
    "                        #line=dict(color='blue', width=2, dash='dot'),\n",
    "                        opacity=0.6\n",
    "                         ))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x, y=f(x,max_params[0],max_params[1]), name='Fit',\n",
    "                        mode='lines',\n",
    "                        line=dict(color='orange', width=5, dash='dot'),\n",
    "                        opacity=1\n",
    "                         ))\n",
    "\n",
    "fig.update_layout(title=\"Distribution (nbins= \"+str(nbins)+\") and the previous fit\",\n",
    "                xaxis_title=\"Distance [1e6 m]\",\n",
    "                yaxis_title=\"Visitors 1e6\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " As we can see even if we increase the number of bins the fit remains qulitatively valid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "As we can see in the plots there are points that are distant from our fit. As said before this discrepances should be due to the distribution of italian population that is not uniform but concentrated in the big cities. In this view we can identify the points that stay above the fit as the big city and that even if are far from Padova have a large number of visitors; the points that stay under the fit as the areas near Padova that have a low concentration of population.\n",
    "\n",
    "#The next plot summarize these conlusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins=200\n",
    "bins, f_bins = distribution(nbins,dist_users_day)\n",
    "bins/=1e6\n",
    "f_bins=np.array(f_bins)/1e6\n",
    "\n",
    "x=np.linspace(np.min(bins),np.max(bins),200)\n",
    "\n",
    "sigma_ab=np.square(np.diag(params_covariance))\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "\n",
    "\n",
    "fig.add_trace(go.Scatter(x=bins, y=f_bins, name='Distribution',\n",
    "                        mode='markers',\n",
    "                        #line=dict(color='blue', width=2, dash='dot'),\n",
    "                        opacity=0.6\n",
    "                         ))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x, y=f(x,max_params[0],max_params[1]), name='Fit',\n",
    "                        mode='lines',\n",
    "                        line=dict(color='orange', width=5, dash='dot'),\n",
    "                        opacity=1\n",
    "                         ))\n",
    "\n",
    "fig.update_layout(title=\"Discrepances between fit and distribution (200 bins)\",\n",
    "                xaxis_title=\"Distance [1e6 m]\",\n",
    "                yaxis_title=\"Visitors 1e6\")\n",
    "\n",
    "\n",
    "fig.add_shape(type=\"circle\",\n",
    "    xref=\"x\", yref=\"y\",\n",
    "    x0=0.07, y0=-0.01,\n",
    "    x1=0.17, y1=0.03,\n",
    "    opacity=0.2,\n",
    "    fillcolor=\"red\",\n",
    "    line_color=\"red\",\n",
    ")\n",
    "\n",
    "big=[['Milano',0.256,0.205],\n",
    "    ['Roma',0.507,0.095],\n",
    "    ['Napoli',.699,.026],\n",
    "    ['Torino',.393,.041],\n",
    "    ['Brescia',0.18,.1]]\n",
    "    #['Venezia + Verona',.076,.62]]\n",
    "\n",
    "\n",
    "for b in big:\n",
    "    fig.add_annotation(x=b[1], y=b[2],\n",
    "            text=b[0],\n",
    "            showarrow=False,\n",
    "            yshift=12,\n",
    "            font=dict(color='red',size=15)\n",
    "    )\n",
    "\n",
    "fig.update_xaxes(range=[0, 0.75])\n",
    "\n",
    "fig.add_annotation(x=0.1, y=.01,\n",
    "            text=\"Under the fit\",\n",
    "            showarrow=True,\n",
    "            arrowhead=2,\n",
    "            font=dict(color='red',size=15),\n",
    "            arrowcolor='red'\n",
    ")\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point 6\n",
    "\n",
    "### Milocco Riccardo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flow_month_reg(data_reg):\n",
    "    flow_m_reg=data_reg.groupby(['MONTH']).FLOW.sum().reset_index()\n",
    "    return flow_m_reg\n",
    "\n",
    "d_flow_month_reg = {} \n",
    "\n",
    "for i in range(len(reg)):\n",
    "    d_flow_month_reg[reg[i]] = pd.DataFrame(flow_month_reg(data_reg[reg[i]]))\n",
    "    \n",
    "def flow_month_pro(data_province_selected, cod_pro_selected):\n",
    "    flow_m=data_province_selected.groupby(['MONTH', 'COD_PRO']).FLOW.sum().reset_index()\n",
    "    return flow_m\n",
    "\n",
    "d_flow_month_pro = {} \n",
    "\n",
    "for i in range(len(reg)):\n",
    "    d_flow_month_pro[reg[i]] = pd.DataFrame(flow_month_pro(d_selected_province[reg[i]],flow_pro[reg[i]]['COD_PRO']))\n",
    "\n",
    "\n",
    "#reconcatenate\n",
    "df=d_flow_month_pro[reg[0]].merge(flow_pro[reg[0]], on=\"COD_PRO\")\n",
    "df[\"Regions\"]=reg[0]\n",
    "for i in range(1,len(reg)):\n",
    "    df=pd.concat([df, d_flow_month_pro[reg[i]].merge(flow_pro[reg[i]], on=\"COD_PRO\")], ignore_index=True)\n",
    "    df=df.replace(np.nan, reg[i])\n",
    "df.rename(columns={\"FLOW_x\":\"FLOW\", \"NAME\":\"Province\", \"FLOW_y\":\"FLOW TOTAL\"}, inplace=True)\n",
    "\n",
    "\n",
    "df[\"MONTH\"] = pd.Categorical(df[\"MONTH\"],\n",
    "                    categories=[\"Febbraio\",\"Marzo\",\"Aprile\",\"Maggio\"])\n",
    "df.sort_values(by=[\"MONTH\",\"FLOW\"], inplace=True, ignore_index=True)\n",
    "total=d_flow_month_pro[\"Lombardia\"].FLOW.sum()\n",
    "\n",
    "\n",
    "#Plot\n",
    "color=['#b32d00', '#fb9804' ,'#b5de2b'][::-1]\n",
    "\n",
    "import plotly.express as px\n",
    "fig = px.bar(df, x = df[\"MONTH\"], y = df[\"FLOW\"], color=df[\"Regions\"], color_discrete_sequence=color, text=df[\"Province\"], title=\"Visitors of nearby regions on months\")\n",
    "#fig.update_xaxes(categoryorder=\"total ascending\")\n",
    "fig.show()\n",
    "\n",
    "fig = px.bar(df, x = df[\"Regions\"], y = df[\"FLOW\"], color=df[\"MONTH\"], color_discrete_sequence=px.colors.qualitative.D3, text=df[\"Province\"], title=\"Visitors of nearby regions on months\")\n",
    "fig.update_xaxes(categoryorder=\"total ascending\")\n",
    "fig.show()\n",
    "\n",
    "\n",
    "#Display the data\n",
    "#tab_months= ff.create_table(df,colorscale=table_color)\n",
    "#tab_months.update_layout(xaxis = {'domain': [.1, .9]}, yaxis1 = {'domain': [0, 1]})\n",
    "#tab_months.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End Part 6.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Riccardo-Michele?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regression function fits well the counting of the visitors but it doesn't grasp the features of the peaks, i.e. the fluctuations with respect to the mean trend. To recover the full behaviour, we compare the bin-count of the visitors and the population curves and we suppose that a possible enanchement could be obtained by join the regression counting of visitors with the regression previously obtained. We evententually point out that the following graphs are cutted, leading to a better visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances=codist_ven\n",
    "dud=dist_users_day\n",
    "\n",
    "filename=r\"data/abitanti2.txt\"\n",
    "abitanti = pd.read_csv(filename, header=None,quoting=2,encoding=\"latin-1\")\n",
    "abitanti = pd.DataFrame(abitanti[0].str.split('\\t' ,n=1).tolist())\n",
    "abitanti.columns = [\"PROVINCIA\", \"VALUE\"]\n",
    "abitanti['VALUE'] = abitanti['VALUE'].apply(pd.to_numeric).sort_values(ascending=False)\n",
    "abitanti=abitanti.merge(codist_prov, on=\"PROVINCIA\")\n",
    "\n",
    "#This cell may be different from the one of Filippo's\n",
    "distances.columns=[\"COD_PRO\",\"Destinazione\",\"Total_Mete\"]\n",
    "abitanti=abitanti.merge(distances, on=\"COD_PRO\")[[\"VALUE\",\"Total_Mete\", \"COD_PRO\", \"PROVINCIA\"]]\n",
    "abitanti=abitanti[abitanti.COD_PRO!=28].sort_values(by=\"Total_Mete\")[[\"VALUE\",\"Total_Mete\"]].reset_index(drop=True)/1e6\n",
    "\n",
    "##############RESCALE dud HERE\n",
    "dud[[\"DISTANCES\",\"VISITORS\"]]=dud[[\"DISTANCES\",\"VISITORS\"]]/1e6\n",
    "\n",
    "dud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###set nbins and refitting the visitors binning\n",
    "nbins=80\n",
    "bins, f_bins = distribution(nbins,dud)\n",
    "f_bins=np.array(f_bins)\n",
    "\n",
    "max_params, params_covariance = optimize.curve_fit(f, bins, f_bins, p0=[1, 1])\n",
    "def f_reg(x): return f(x, max_params[0], max_params[1])\n",
    "\n",
    "def f_joined(x): \n",
    "    return f_reg(x)*abitanti[abitanti.Total_Mete==x].VALUE.to_numpy()[0]\n",
    "f_joined=np.vectorize(f_joined)\n",
    "\n",
    "def inhabits(x): return abitanti[abitanti.Total_Mete==x].VALUE.to_numpy()[0]\n",
    "inhabits=np.vectorize(inhabits)\n",
    "\n",
    "d=abitanti.sort_values(by=\"Total_Mete\").Total_Mete.to_numpy()\n",
    "\n",
    "#important: set the variables /1e6 since f_reg is for that scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting part\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "trace1 = go.Scatter(x=bins, y=f_bins, name='Visitors for %s bins' % nbins,\n",
    "                        mode='lines+markers',\n",
    "                        line=dict(color='blue', width=1, dash='dot'),\n",
    "                        marker=dict(size=4)\n",
    "                         )\n",
    "\n",
    "trace2 = go.Scatter(x=d, y=f_joined(d), name=\"Fit + Inhabitants\",\n",
    "                        mode='lines+markers',\n",
    "                        line=dict(color='red', width=1, dash='dot'),\n",
    "                        marker=dict(size=3)\n",
    "                         )\n",
    "\n",
    "trace3 = go.Scatter(x=d, y=inhabits(d), name=\"Inhabitants\",\n",
    "                    mode='lines',\n",
    "                    line=dict(color='teal', width=1),\n",
    ")\n",
    "\n",
    "trace32 = go.Scatter(x=d, y=inhabits(d), name=\"Inhabitants\",\n",
    "                    mode='lines',\n",
    "                    line=dict(color='teal', width=1),\n",
    "                    xaxis='x2',\n",
    "                    yaxis='y2'\n",
    ")\n",
    "\n",
    "trace4 = go.Scatter(x=bins, y=f_reg(bins), name=\"Fit\",\n",
    "                        mode='lines',\n",
    "                        line=dict(color='orange', width=1, dash='dot'),\n",
    "                        marker=dict(size=3)\n",
    "                         )\n",
    "\n",
    "xaxis_title=\"Distance [1e3*Km]\"\n",
    "margin=dict(l=50,r=50,b=0,t=100,pad=4)\n",
    "width,height=800,400\n",
    "\n",
    "data1=[trace1, trace3]\n",
    "fig1 = go.Figure(data1)\n",
    "\n",
    "fig1.update_layout(title=\"Distribution of Individuals as a function of distance from Padua \",\n",
    "                xaxis_title=xaxis_title,\n",
    "                yaxis_title=\"Individuals[1e6]\", \n",
    "                margin=margin, width=width, height=height\n",
    ")\n",
    "\n",
    "fig1.update_yaxes(title_standoff = 0, mirror=True, linecolor=\"black\",gridcolor='LightGrey'),\n",
    "fig1.update_xaxes(title_standoff = 10, mirror=True, \n",
    "                range=[0.01,1],linecolor=\"black\", gridcolor=\"LightGrey\",tick0=0.25, dtick=0.25)\n",
    "\n",
    "big1=[[\"Rovigo\",0.09,-f_bins[5]*5],\n",
    "    ['Milano',0.256,-5],\n",
    "    ['Roma',0.507,-5],\n",
    "    ['Napoli',.699,-5],\n",
    "    ['Torino',.393,-5],\n",
    "    ['Brescia',0.18,-5],\n",
    "    ['VE + VR',.1,-.1]]\n",
    "\n",
    "for b in big1:\n",
    "    fig1.add_annotation(x=b[1], y=b[2],\n",
    "            text=b[0],\n",
    "            showarrow=False,\n",
    "            yshift=12,\n",
    "            font=dict(color='darkgreen',size=10)\n",
    "    )\n",
    "\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The previous claim is well summarized in the next plot, where we can recover a better connection between avaible data and our estimate. Moreover, it may be seen that not all of the peaks are recovered, since the dropping in the number of visitors due to the regression is not sustained by the inhabitants living at a certain distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = [trace1, trace2, trace32]\n",
    "layout = go.Layout(\n",
    "                xaxis2=dict(domain=[0.5, 0.95],anchor='y2'),\n",
    "                yaxis2=dict(domain=[0.5, 0.95],anchor='x2'),\n",
    ")\n",
    "\n",
    "fig2 = go.Figure(data=data2, layout=layout)\n",
    "\n",
    "fig2.update_layout(title=\"Visitors for %s bins\" %nbins,\n",
    "                xaxis_title=xaxis_title,\n",
    "                yaxis_title=\"Visitors [1e6]\",\n",
    "                xaxis2_title=\"Distance [1e3*Km]\",\n",
    "                yaxis2_title=\"Inhabitants [1e6]\",\n",
    "                legend=dict(x=0.1,y=0.95,traceorder='normal',font=dict(size=12,),),\n",
    "                margin=margin, width=width, height=height\n",
    "                )\n",
    "\n",
    "fig2.update_yaxes(title_standoff = 0, mirror=True, linecolor=\"black\",gridcolor='LightGrey'),\n",
    "fig2.update_xaxes(title_standoff = 10, mirror=True, \n",
    "                range=[0.05,0.75],linecolor=\"black\", gridcolor=\"LightGrey\")\n",
    "\n",
    "\n",
    "big=[[\"Rovigo\",0.09,1.2],\n",
    "    ['Milano',0.256,0.205],\n",
    "    ['Roma',0.507,0.095],\n",
    "    ['Napoli',.699,.026],\n",
    "    ['Torino',.393,.041],\n",
    "    ['Brescia',0.18,.1],\n",
    "    ['VE + VR',.1,.62]]\n",
    "\n",
    "\n",
    "for b in big:\n",
    "    fig2.add_annotation(x=b[1], y=b[2],\n",
    "            text=b[0],\n",
    "            showarrow=False,\n",
    "            yshift=12,\n",
    "            font=dict(color='darkgreen',size=10)\n",
    "    )\n",
    "\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other way around, by counting the number of visitors reaching Padua, we may estimate the population distribution via $N_{inhab}(d)=(N_{visitors}/N_{regression})(d)$.\n",
    "As expected the smallness of the regression after $200Km$, is responsable for the deviation from the expected results.\n",
    "A special understanding of this, could be gained by looking at the following plot, where the graph is not limited at the big distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = go.Figure()\n",
    "fig2.add_trace(go.Scatter(x=abitanti.Total_Mete, y=abitanti.VALUE, name='Real',\n",
    "                        mode='lines',\n",
    "                        line=dict(color='teal', width=2),\n",
    "                        opacity=0.6\n",
    "                         ))\n",
    "\n",
    "fig2.add_trace(go.Scatter(x=bins, y=f_bins/f_reg(bins), name='Syntetic',\n",
    "                        mode='lines',\n",
    "                        line=dict(color='red', width=2, dash='dot'),\n",
    "                        opacity=1\n",
    "                         ))\n",
    "fig2.update_layout(title=\"Syntetic Population Distribution \",\n",
    "                yaxis_title=\"Population\",\n",
    "                xaxis_title=\"Distance [1e3*Km]\", legend=dict(x=0.1,y=0.95,traceorder='normal',font=dict(size=12,),),\n",
    "                margin=margin, width=width, height=height,\n",
    ")\n",
    "fig2.update_xaxes(title_standoff = 10, mirror=True, \n",
    "range=[0,1.2],linecolor=\"black\", gridcolor=\"LightGrey\")\n",
    "fig2.update_yaxes(title_standoff = 0, mirror=True, linecolor=\"black\",gridcolor='LightGrey'),\n",
    "\n",
    "big=[[\"Rovigo\",0.09,-5],\n",
    "    ['Milano',0.256,-5],\n",
    "    ['Roma',0.507,-5],\n",
    "    ['Napoli',.699,-5],\n",
    "    ['Torino',.393,-5],\n",
    "    ['Brescia',0.18,-5],\n",
    "    ['VE + VR',.1,-.1]]\n",
    "\n",
    "for b in big:\n",
    "    fig2.add_annotation(x=b[1], y=b[2],\n",
    "            text=b[0],\n",
    "            showarrow=False,\n",
    "            yshift=12,\n",
    "            font=dict(color='darkgreen',size=10)\n",
    "    )\n",
    "\n",
    "fig2.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
